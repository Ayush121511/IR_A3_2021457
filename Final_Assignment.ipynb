{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8000583,"sourceType":"datasetVersion","datasetId":4711299},{"sourceId":8041089,"sourceType":"datasetVersion","datasetId":4740783},{"sourceId":8048801,"sourceType":"datasetVersion","datasetId":4746352}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:52:21.762039Z","iopub.execute_input":"2024-04-01T17:52:21.762401Z","iopub.status.idle":"2024-04-01T17:52:21.78991Z","shell.execute_reply.started":"2024-04-01T17:52:21.762375Z","shell.execute_reply":"2024-04-01T17:52:21.788997Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport gzip\nimport json\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n# from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport string\nimport re\n\nimport pandas as pd\nimport gzip\nimport json\n# Define the path to your .json.gz file\njson_gz_file_path = '/kaggle/input/amazon-metadata/meta_Electronics.json'\n\n# Define chunk size\nchunksize = 10000  # Adjust this based on your system's capacity\n\n# Open the compressed file and read it as a Pandas DataFrame in chunks\nchunks = []\nwith open(json_gz_file_path, 'rb') as f:\n    for chunk in pd.read_json(f, lines=True, chunksize=chunksize):\n        chunks.append(chunk)\n\n# Concatenate all chunks into a single DataFrame\nmeta_data = pd.concat(chunks, ignore_index=True)\n\n# Define the path to your .json.gz file\njson_gz_file_path = '/kaggle/input/electronics-dataset/Electronics_5.json'\n\n# Define chunk size\nchunksize = 10000  # Adjust this based on your system's capacity\n\n# Open the compressed file and read it as a Pandas DataFrame in chunks\nchunks = []\nwith open(json_gz_file_path, 'rb') as f:\n    for chunk in pd.read_json(f, lines=True, chunksize=chunksize):\n        chunks.append(chunk)\n\n# Concatenate all chunks into a single DataFrame\ndf = pd.concat(chunks, ignore_index=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-06T17:16:47.500594Z","iopub.execute_input":"2024-04-06T17:16:47.500999Z","iopub.status.idle":"2024-04-06T17:19:38.264764Z","shell.execute_reply.started":"2024-04-06T17:16:47.500959Z","shell.execute_reply":"2024-04-06T17:19:38.263658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2","metadata":{}},{"cell_type":"code","source":"\n\ndef cosine_similarity(X, Y):\n\n    # Compute dot product\n    dot_product = np.dot(X, Y.T)\n\n    # Compute norms\n    norm_X = np.linalg.norm(X, axis=1)\n    norm_Y = np.linalg.norm(Y, axis=1)\n\n    # Compute cosine similarity\n    similarity = dot_product / (np.outer(norm_X, norm_Y) + 1e-10)  # Adding a small value to avoid division by zero\n\n    return similarity\n\ndef find_similar_users_2(user_id, rating_matrix, val_vector, top_n):\n    # Calculate cosine similarity between the given user (val_vector) and all users in the training set (rating_matrix)\n    similarity = cosine_similarity(rating_matrix, val_vector.reshape(1, -1))\n    \n    # Sort the similarities and get the indices of top_n similar users\n    similar_users = np.argsort(similarity[:, 0])[::-1][:top_n]\n    \n    return similar_users\n\ndef user_user_recommendation(rating_matrix, k_folds, top_ns):\n    kf = KFold(n_splits=k_folds)\n    mae_values = []\n    \n    for top_n in top_ns:\n        fold_mae = []\n        for train_index, val_index in kf.split(rating_matrix):\n            train_set = rating_matrix.iloc[train_index]\n            val_set = rating_matrix.iloc[val_index]\n            # print(train_set.shape, val_set.shape)\n            predicted_ratings = np.zeros_like(val_set)\n            val_ratings = np.zeros_like(val_set)\n            count = 0\n            for user_id, user_ratings in val_set.iterrows():\n                # print(count, \"\\\\\", len(val_set))\n                similar_users = find_similar_users_2(user_id, train_set,user_ratings.values, top_n)\n                avg_rating = train_set.iloc[similar_users].mean(axis=0)\n                predicted_ratings[count] = avg_rating.values\n                val_ratings[count] = user_ratings.values\n                count += 1\n\n                # print(\"--\")\n            break\n                \n        mae_values.append(mean_absolute_error(val_ratings[val_ratings != 0], predicted_ratings[val_ratings != 0]))\n        print(top_n ,\"-Done\")\n    return mae_values\n\n\ndef item_item_recommendation(rating_matrix, k_folds, top_ns):\n    kf = KFold(n_splits=k_folds)\n    mae_values = []\n    \n    for top_n in top_ns:\n        fold_mae = []\n        rating_matrix_transposed = rating_matrix.T\n        # print(rating_matrix_transposed.info())\n        for train_index, val_index in kf.split(rating_matrix_transposed):\n            train_set = rating_matrix_transposed.iloc[train_index]\n            val_set = rating_matrix_transposed.iloc[val_index]\n            predicted_ratings = np.zeros_like(val_set)\n            val_ratings = np.zeros_like(val_set)\n            count = 0\n            for user_id, user_ratings in val_set.iterrows():\n\n                similar_users = find_similar_users_2(user_id, train_set,user_ratings.values, top_n)\n                avg_rating = train_set.iloc[similar_users].mean(axis=0)\n                predicted_ratings[count] = avg_rating.values\n                val_ratings[count] = user_ratings.values\n                count += 1\n            break\n                \n        mae_values.append(mean_absolute_error(val_ratings[val_ratings != 0], predicted_ratings[val_ratings != 0]))\n        print(top_n ,\"-Done\")\n    return mae_values\n\n# Function to convert lists and dictionaries to tuples\ndef convert_to_tuple(value):\n    if isinstance(value, list):\n        return tuple(value)\n    elif isinstance(value, dict):\n        return tuple(value.items())\n    else:\n        return value","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_name = \"Headphones\"","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:39:59.144394Z","iopub.execute_input":"2024-04-06T17:39:59.144809Z","iopub.status.idle":"2024-04-06T17:39:59.149511Z","shell.execute_reply.started":"2024-04-06T17:39:59.14477Z","shell.execute_reply":"2024-04-06T17:39:59.14842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3","metadata":{}},{"cell_type":"code","source":"\n# Apply the conversion function to each element in the DataFrame\nmeta2 = meta_data.applymap(convert_to_tuple)\n\n# Replace NaN values with a placeholder\nmeta2.fillna('NaN', inplace=True)\n\n# Remove duplicates\nmeta2.drop_duplicates(inplace=True)\n\n# Convert 'NaN' placeholder back to actual NaN values\nmeta2.replace('NaN', np.nan, inplace=True)\n\nfrom PIL import Image, ImageEnhance","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:53.175661Z","iopub.execute_input":"2024-04-06T17:43:53.17605Z","iopub.status.idle":"2024-04-06T17:43:56.567834Z","shell.execute_reply.started":"2024-04-06T17:43:53.176022Z","shell.execute_reply":"2024-04-06T17:43:56.566526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\n# Function to convert lists and dictionaries to tuples\ndef convert_to_tuple(value):\n    if isinstance(value, list):\n        return tuple(value)\n    elif isinstance(value, dict):\n        return tuple(value.items())\n    else:\n        return value\n\n# Apply the conversion function to each element in the DataFrame\ndf = df.applymap(convert_to_tuple)\n\n# Replace NaN values with a placeholder\ndf.fillna('NaN', inplace=True)\n\n# Remove duplicates\ndf.drop_duplicates(inplace=True)\n\n# Convert 'NaN' placeholder back to actual NaN values\ndf.replace('NaN', np.nan, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:48:07.879954Z","iopub.execute_input":"2024-04-06T17:48:07.880367Z","iopub.status.idle":"2024-04-06T17:50:40.792772Z","shell.execute_reply.started":"2024-04-06T17:48:07.880339Z","shell.execute_reply":"2024-04-06T17:50:40.791631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_meta_data_rows = []\ni = 0\nfor index, row in meta2.iterrows():\n    if product_name in row[\"title\"]:\n        product_meta_data_rows.append(i)\n    i+=1\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:56:46.10938Z","iopub.execute_input":"2024-04-06T17:56:46.110124Z","iopub.status.idle":"2024-04-06T17:57:33.164665Z","shell.execute_reply.started":"2024-04-06T17:56:46.110075Z","shell.execute_reply":"2024-04-06T17:57:33.163666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m2 = meta2.iloc[product_meta_data_rows].dropna(subset = [\"asin\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:58:26.710159Z","iopub.execute_input":"2024-04-06T17:58:26.710653Z","iopub.status.idle":"2024-04-06T17:58:26.846737Z","shell.execute_reply.started":"2024-04-06T17:58:26.710615Z","shell.execute_reply":"2024-04-06T17:58:26.845576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_data =  pd.merge(m2, df, on='asin', how='inner')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:20:43.428389Z","iopub.execute_input":"2024-04-06T19:20:43.42883Z","iopub.status.idle":"2024-04-06T19:20:49.710121Z","shell.execute_reply.started":"2024-04-06T19:20:43.428798Z","shell.execute_reply":"2024-04-06T19:20:49.709096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4","metadata":{}},{"cell_type":"code","source":"print(\"Number of Reviews:\",len(product_data))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:21:08.614389Z","iopub.execute_input":"2024-04-06T19:21:08.615936Z","iopub.status.idle":"2024-04-06T19:21:08.623888Z","shell.execute_reply.started":"2024-04-06T19:21:08.615866Z","shell.execute_reply":"2024-04-06T19:21:08.622583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_score = sum( product_data[\"overall\"].values )/len(product_data)\nprint(\"Average Rating Score:\",avg_score)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:05:21.523946Z","iopub.execute_input":"2024-04-06T18:05:21.524552Z","iopub.status.idle":"2024-04-06T18:05:21.579269Z","shell.execute_reply.started":"2024-04-06T18:05:21.524521Z","shell.execute_reply":"2024-04-06T18:05:21.578412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Unique Products:\",len(product_data[\"asin\"].unique()))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:06:12.402623Z","iopub.execute_input":"2024-04-06T18:06:12.403212Z","iopub.status.idle":"2024-04-06T18:06:12.435152Z","shell.execute_reply.started":"2024-04-06T18:06:12.403183Z","shell.execute_reply":"2024-04-06T18:06:12.433823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Good Ratings:\", sum( [ 1 for index, row in product_data.iterrows() if row[\"overall\"] >=3 ] ) )","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:07:43.642871Z","iopub.execute_input":"2024-04-06T18:07:43.643242Z","iopub.status.idle":"2024-04-06T18:07:59.54578Z","shell.execute_reply.started":"2024-04-06T18:07:43.643212Z","shell.execute_reply":"2024-04-06T18:07:59.544547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Bad Ratings:\", len(product_data) - sum( [ 1 for index, row in product_data.iterrows() if row[\"overall\"] >=3 ] ) )","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:08:36.100285Z","iopub.execute_input":"2024-04-06T18:08:36.100891Z","iopub.status.idle":"2024-04-06T18:08:52.089211Z","shell.execute_reply.started":"2024-04-06T18:08:36.10086Z","shell.execute_reply":"2024-04-06T18:08:52.088284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Reviews for each rating:\")\ni = 0\nwhile i < 6:\n    count = 0\n    index = 0\n    while index < len(product_data):\n        row = product_data.iloc[index]\n        if row[\"overall\"] == i:\n            count += 1\n        index += 1\n    print(f\"Rating {i}: {count}\")\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:11:39.381388Z","iopub.execute_input":"2024-04-06T18:11:39.381809Z","iopub.status.idle":"2024-04-06T18:13:14.92975Z","shell.execute_reply.started":"2024-04-06T18:11:39.381776Z","shell.execute_reply":"2024-04-06T18:13:14.928594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\ndef remove_accented_chars(text):\n    if pd.isna(text):\n        return text  # If NaN, return NaN\n    return unidecode(text)\n\n\n# Function to remove HTML tags\ndef remove_html_tags(text):\n    if pd.isna(text):\n        return text  # If NaN, return NaN\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)\n\n# Function to remove special characters\ndef remove_special_characters(text):\n    if pd.isna(text):\n        return text\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\ndef lemmatize_text(text):\n    if pd.isna(text):\n        return text\n    doc = nlp(text)\n    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n    return lemmatized_text\n\n\n# Apply the function to the column\nproduct_data['reviewText'] = product_data['reviewText'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:21:12.415098Z","iopub.execute_input":"2024-04-06T19:21:12.415691Z","iopub.status.idle":"2024-04-06T19:21:13.632196Z","shell.execute_reply.started":"2024-04-06T19:21:12.415652Z","shell.execute_reply":"2024-04-06T19:21:13.630971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom unidecode import unidecode\n\n\n# Apply the function to the column\nproduct_data['reviewText'] = product_data['reviewText'].apply(remove_accented_chars)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:22:47.762713Z","iopub.execute_input":"2024-04-06T19:22:47.763246Z","iopub.status.idle":"2024-04-06T19:22:48.25308Z","shell.execute_reply.started":"2024-04-06T19:22:47.763209Z","shell.execute_reply":"2024-04-06T19:22:48.251596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:22:25.383932Z","iopub.execute_input":"2024-04-06T18:22:25.384354Z","iopub.status.idle":"2024-04-06T18:22:41.787121Z","shell.execute_reply.started":"2024-04-06T18:22:25.384322Z","shell.execute_reply":"2024-04-06T18:22:41.786054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport contractions\n\n# Function to expand acronyms\ndef expand_acronyms(text):\n    if pd.isna(text):\n        return text\n    return contractions.fix(text)\n\nimport pandas as pd\nimport spacy\n\n# Load spaCy English model\nnlp = spacy.load('en_core_web_sm')\n\n# Apply the function to the column\nproduct_data['reviewText'] = product_data['reviewText'].apply(expand_acronyms)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:24:35.617216Z","iopub.execute_input":"2024-04-06T19:24:35.617656Z","iopub.status.idle":"2024-04-06T19:24:48.398681Z","shell.execute_reply.started":"2024-04-06T19:24:35.617624Z","shell.execute_reply":"2024-04-06T19:24:48.397442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n\n\n# Apply the function to the column\nproduct_data['reviewText'] = product_data['reviewText'].apply(remove_special_characters)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:25:13.245279Z","iopub.execute_input":"2024-04-06T19:25:13.245733Z","iopub.status.idle":"2024-04-06T19:25:18.563585Z","shell.execute_reply.started":"2024-04-06T19:25:13.2457Z","shell.execute_reply":"2024-04-06T19:25:18.561863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Function to perform lemmatization\n\n# Apply the function to the column\nproduct_data['reviewText'] = product_data['reviewText'].apply(lemmatize_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T20:22:00.083038Z","iopub.execute_input":"2024-04-06T20:22:00.083645Z","iopub.status.idle":"2024-04-06T21:00:01.118921Z","shell.execute_reply.started":"2024-04-06T20:22:00.083604Z","shell.execute_reply":"2024-04-06T21:00:01.116541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Download NLTK stopwords (run this line only once)\nimport nltk\nnltk.download('stopwords')\n\n# Load English stopwords\nstop_words = set(stopwords.words('english'))\n\n# Function to remove stopwords\ndef remove_stopwords(text):\n    if pd.isna(text):\n        return text\n    tokens = word_tokenize(text)\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n    return ' '.join(filtered_tokens)\n\n# Apply the function to the column\nproduct_data['reviewText'] = product_data['reviewText'].apply(remove_stopwords)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:32:02.509167Z","iopub.execute_input":"2024-04-06T19:32:02.509733Z","iopub.status.idle":"2024-04-06T19:35:27.903503Z","shell.execute_reply.started":"2024-04-06T19:32:02.509693Z","shell.execute_reply":"2024-04-06T19:35:27.902123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nprint(\"Top 20 most reviewed brands:\")\nbrand_reviews = { brand : 0 for brand in product_data[\"brand\"].unique() }\n\nfor index, row in product_data.iterrows():\n    brand_reviews[row[\"brand\"]]+=1\n\ntop_20_brands = sorted( brand_reviews.items(), key = lambda x: x[1], reverse = True )[:20]\n\nfor brand, number_of_reviews in top_20_brands:\n    print(brand,number_of_reviews)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:53:23.313564Z","iopub.execute_input":"2024-04-06T18:53:23.314769Z","iopub.status.idle":"2024-04-06T18:53:39.381739Z","shell.execute_reply.started":"2024-04-06T18:53:23.314733Z","shell.execute_reply":"2024-04-06T18:53:39.380617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b2 = sorted( brand_reviews.items(), key = lambda x: x[1])[0:20]\nprint(\"Last 20 reviewed brands:\")\nfor brand, number_of_reviews in b2:\n    print(brand,number_of_reviews)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:56:18.9923Z","iopub.execute_input":"2024-04-06T18:56:18.993512Z","iopub.status.idle":"2024-04-06T18:56:19.001023Z","shell.execute_reply.started":"2024-04-06T18:56:18.993467Z","shell.execute_reply":"2024-04-06T18:56:19.000018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\naverage_ratings = product_data.groupby('title')['overall'].mean()\n\nhighest_rated_title = average_ratings.idxmax()\n\nprint(\"Headphones with the highest average rating:\", highest_rated_title)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:10:25.50134Z","iopub.execute_input":"2024-04-06T19:10:25.50243Z","iopub.status.idle":"2024-04-06T19:10:25.581929Z","shell.execute_reply.started":"2024-04-06T19:10:25.502389Z","shell.execute_reply":"2024-04-06T19:10:25.580895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_reviews = product_data[product_data['overall'] >= 3]['reviewText']\nbad_reviews = product_data[product_data['overall'] < 3]['reviewText']\n\n# Tokenize the reviews into words\ndef tokenize_reviews(reviews):\n    tokens = []\n    for review in reviews:\n        if pd.isna(review):\n            tokens.extend([])\n        else:\n            tokens.extend(word_tokenize(review.lower()))\n    return tokens\n\n# Tokenize, remove stopwords and punctuation, and count the frequency of each word\ngood_tokens = tokenize_reviews(good_reviews)\nbad_tokens = tokenize_reviews(bad_reviews)\n\n\n# Generate frequency distributions\ngood_freq_dist = FreqDist(good_tokens)\nbad_freq_dist = FreqDist(bad_tokens)\n\n# Generate word clouds\ngood_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(good_freq_dist)\nbad_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bad_freq_dist)\n\n# Plot word clouds\nplt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(good_wordcloud, interpolation='bilinear')\nplt.title('Word Cloud for Good Ratings')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(bad_wordcloud, interpolation='bilinear')\nplt.title('Word Cloud for Bad Ratings')\nplt.axis('off')\n\nplt.show()\n\n# Report most commonly used words for positive and negative reviews\nprint(\"Most commonly used words for positive reviews:\")\nprint(good_freq_dist.most_common(10))\n\nprint(\"\\nMost commonly used words for negative reviews:\")\nprint(bad_freq_dist.most_common(10))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T20:04:28.129775Z","iopub.execute_input":"2024-04-06T20:04:28.131298Z","iopub.status.idle":"2024-04-06T20:07:00.93574Z","shell.execute_reply.started":"2024-04-06T20:04:28.13125Z","shell.execute_reply":"2024-04-06T20:07:00.93468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings_number_of_reviews = dict()\ni = 1\nwhile i <= 5:\n    count = 0\n    index = 0\n    while index < len(product_data):\n        row = product_data.iloc[index]\n        if row[\"overall\"] == i:\n            count += 1\n        index += 1\n    ratings_number_of_reviews[i] = count\n    i += 1\n\ndata = ratings_number_of_reviews\n# Get keys and values\nlabels = data.keys()\nsizes = data.values()\n\n# Plotting the pie chart\nplt.figure(figsize=(8, 8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Ratings vs Number of Reviews')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:04:36.573745Z","iopub.execute_input":"2024-04-06T21:04:36.57463Z","iopub.status.idle":"2024-04-06T21:05:56.664529Z","shell.execute_reply.started":"2024-04-06T21:04:36.574592Z","shell.execute_reply":"2024-04-06T21:05:56.662982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extractYear(time):\n    return int(time.split(\",\")[1].strip())\nreview_years = product_data[\"reviewTime\"].apply(extractYear)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:06:57.836817Z","iopub.execute_input":"2024-04-06T21:06:57.8372Z","iopub.status.idle":"2024-04-06T21:06:58.207983Z","shell.execute_reply.started":"2024-04-06T21:06:57.837169Z","shell.execute_reply":"2024-04-06T21:06:58.206802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_data[\"reviewYear\"] = review_years","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:18:00.118872Z","iopub.execute_input":"2024-04-06T21:18:00.119355Z","iopub.status.idle":"2024-04-06T21:18:00.127364Z","shell.execute_reply.started":"2024-04-06T21:18:00.119321Z","shell.execute_reply":"2024-04-06T21:18:00.125948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year_number_of_reviews = {year: 0 for year in review_years.unique()}\nindex = 0\nwhile index < len(review_years):\n    yn = review_years.iloc[index]\n    year_number_of_reviews[yn] += 1\n    index += 1\n\nyears = list(year_number_of_reviews.keys())\ntop_year = years[0]\nmax_reviews = year_number_of_reviews[top_year]\nindex = 1\nwhile index < len(years):\n    year = years[index]\n    if year_number_of_reviews[year] > max_reviews:\n        top_year = year\n        max_reviews = year_number_of_reviews[year]\n    index += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:14:24.280828Z","iopub.execute_input":"2024-04-06T21:14:24.281264Z","iopub.status.idle":"2024-04-06T21:14:24.435794Z","shell.execute_reply.started":"2024-04-06T21:14:24.281229Z","shell.execute_reply":"2024-04-06T21:14:24.434528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year_number_of_customers = dict(product_data.groupby(\"reviewYear\")[\"reviewerName\"].unique())","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:23:01.433682Z","iopub.execute_input":"2024-04-06T21:23:01.434744Z","iopub.status.idle":"2024-04-06T21:23:01.547694Z","shell.execute_reply.started":"2024-04-06T21:23:01.434707Z","shell.execute_reply":"2024-04-06T21:23:01.546677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_year = sorted(year_number_of_customers.items(), key = lambda x: len(x[1]),reverse = True)[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:24:27.279494Z","iopub.execute_input":"2024-04-06T21:24:27.279895Z","iopub.status.idle":"2024-04-06T21:24:27.285974Z","shell.execute_reply.started":"2024-04-06T21:24:27.279858Z","shell.execute_reply":"2024-04-06T21:24:27.284715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_year","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:24:28.82452Z","iopub.execute_input":"2024-04-06T21:24:28.824963Z","iopub.status.idle":"2024-04-06T21:24:28.832732Z","shell.execute_reply.started":"2024-04-06T21:24:28.82491Z","shell.execute_reply":"2024-04-06T21:24:28.831505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt')\n\n# Tokenize the text in the 'reviewText' column\ntokenized_reviews = product_data['reviewText'].dropna().apply(word_tokenize)\n\n# Train the Word2Vec model\nmodel = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n\n# Example usage: Get vector representation of a word\nvector = model.wv['product']\n\n# Example usage: Find most similar words\nsimilar_words = model.wv.most_similar('product')\n\nprint(\"Vector representation of 'product':\", vector)\nprint(\"Most similar words to 'product':\", similar_words)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:28:40.707731Z","iopub.execute_input":"2024-04-06T21:28:40.708793Z","iopub.status.idle":"2024-04-06T21:32:04.204124Z","shell.execute_reply.started":"2024-04-06T21:28:40.708748Z","shell.execute_reply":"2024-04-06T21:32:04.202419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_review(text):\n    if pd.isna(text):\n        return []\n    else:\n        return word_tokenize(text)\ndef get_average_embedding(words, model):\n    embeddings = [model.wv[word] for word in words if word in model.wv]\n    if embeddings:\n        return sum(embeddings) / len(embeddings)\n    else:\n        return [0] * model.vector_size  # Return zero vector if none of the words are in the vocabulary\n\n# Get embeddings for each review\nreview_embeddings = [get_average_embedding(review, model) for review in product_data['reviewText'].apply(tokenize_review)]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:59:36.443291Z","iopub.execute_input":"2024-04-06T21:59:36.443789Z","iopub.status.idle":"2024-04-06T22:02:39.003058Z","shell.execute_reply.started":"2024-04-06T21:59:36.443754Z","shell.execute_reply":"2024-04-06T22:02:39.001866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_data[\"reviewTextEmbeddings\"] = review_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:03:49.190099Z","iopub.execute_input":"2024-04-06T22:03:49.190559Z","iopub.status.idle":"2024-04-06T22:03:49.240432Z","shell.execute_reply.started":"2024-04-06T22:03:49.190524Z","shell.execute_reply":"2024-04-06T22:03:49.239233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8","metadata":{}},{"cell_type":"code","source":"def review_class(overall):\n    if overall>3:\n        return \"Good\"\n    elif overall == 3:\n        return \"Average\"\n    else:\n        return \"Bad\"\nproduct_data[\"ratingClass\"] = [review_class(row[\"overall\"]) for index,row in product_data.iterrows()]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:07:57.394536Z","iopub.execute_input":"2024-04-06T22:07:57.395568Z","iopub.status.idle":"2024-04-06T22:08:13.320059Z","shell.execute_reply.started":"2024-04-06T22:07:57.395533Z","shell.execute_reply":"2024-04-06T22:08:13.318924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_data['reviewText'] = product_data['reviewText'].str.replace(',', r'\\,')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:12:36.058639Z","iopub.execute_input":"2024-04-06T22:12:36.059062Z","iopub.status.idle":"2024-04-06T22:12:36.291638Z","shell.execute_reply.started":"2024-04-06T22:12:36.059031Z","shell.execute_reply":"2024-04-06T22:12:36.290262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle as pkl\nwith open(\"Headphones.pkl\",\"wb\") as file:\n    pkl.dump(product_data,file)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:16:21.882898Z","iopub.execute_input":"2024-04-06T22:16:21.883296Z","iopub.status.idle":"2024-04-06T22:16:26.784928Z","shell.execute_reply.started":"2024-04-06T22:16:21.883266Z","shell.execute_reply":"2024-04-06T22:16:26.783674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle as pkl\n\nwith open(\"/kaggle/input/headphones-dataset/Headphones.pkl\",\"rb\") as file:\n    product_data_2 = pkl.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:14:07.793495Z","iopub.execute_input":"2024-04-07T19:14:07.793898Z","iopub.status.idle":"2024-04-07T19:14:16.117565Z","shell.execute_reply.started":"2024-04-07T19:14:07.79387Z","shell.execute_reply":"2024-04-07T19:14:16.1166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ngood_threshold = 4.0\nbad_threshold = 3.0\n\nproduct_df = product_data_2.dropna(subset=['overall', 'reviewText'])\nproduct_df['Rating Class'] = product_df['overall'].apply(lambda x: 'Good' if x >= good_threshold else ('Average' if x == bad_threshold else 'Bad'))\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(product_df['Rating Class'])\n\nlabel_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(\"Label Mapping:\")\nprint(label_mapping)\n\ntfidf_vectorizer = TfidfVectorizer()\nX = tfidf_vectorizer.fit_transform(product_df['reviewText'])\n\n\nindices_y0 = np.where(y == 0)[0]\nindices_y1 = np.where(y == 1)[0]\nindices_y2 = np.where(y == 2)[0]\n\nnum_indices_to_select = int(0.1 * len(indices_y2))\n\nselected_indices_y2 = np.random.choice(indices_y2, size=num_indices_to_select, replace=False)\n\nsampled_indices = np.concatenate([indices_y0, indices_y1, selected_indices_y2])\n\nsampled_X = X[sampled_indices]\nsampled_y = y[sampled_indices]\n# print(sampled_y.shape, sampled_X.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(sampled_X, sampled_y, test_size=0.25, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:14:25.761649Z","iopub.execute_input":"2024-04-07T19:14:25.762181Z","iopub.status.idle":"2024-04-07T19:14:42.71393Z","shell.execute_reply.started":"2024-04-07T19:14:25.762149Z","shell.execute_reply":"2024-04-07T19:14:42.71288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Support Vector Machine': KNeighborsClassifier(n_neighbors=4),\n    'Multinomial Naive Bayes': MultinomialNB(),\n    'Gradient Boosting': GradientBoostingClassifier()\n}\n\nfor name_model, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    report = classification_report(y_test, y_pred, target_names=['Bad', 'Average', 'Good'])\n    print(f'Model: {name_model}')\n    print(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:14:54.415745Z","iopub.execute_input":"2024-04-07T19:14:54.416688Z","iopub.status.idle":"2024-04-07T19:14:58.666185Z","shell.execute_reply.started":"2024-04-07T19:14:54.416652Z","shell.execute_reply":"2024-04-07T19:14:58.664275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_df = product_df.sample(frac=0.1, random_state=42) \nsampled_df.reset_index(drop=True, inplace=True)\n\nsampled_df.info()\nrating_matrix = sampled_df.pivot_table(index='reviewerID', columns='asin', values='overall')\nmin_rating = rating_matrix.min().min()\nmax_rating = rating_matrix.max().max()\nnormalized_rating_matrix = (rating_matrix - min_rating) / (max_rating - min_rating)\nnormalized_rating_matrix = normalized_rating_matrix.fillna(0)\nnon_zero_count = np.count_nonzero(normalized_rating_matrix)\n\nprint(\"Number of non-zero values in the DataFrame:\", non_zero_count)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:15:07.820586Z","iopub.execute_input":"2024-04-07T19:15:07.821011Z","iopub.status.idle":"2024-04-07T19:15:12.553454Z","shell.execute_reply.started":"2024-04-07T19:15:07.820959Z","shell.execute_reply":"2024-04-07T19:15:12.552283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ns = [10, 20, 30, 40, 50]\nk_folds = 5\nuser_mae_values = user_user_recommendation(normalized_rating_matrix, k_folds ,top_ns)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T20:15:10.364113Z","iopub.execute_input":"2024-04-07T20:15:10.364517Z","iopub.status.idle":"2024-04-07T20:15:28.414783Z","shell.execute_reply.started":"2024-04-07T20:15:10.364489Z","shell.execute_reply":"2024-04-07T20:15:28.413148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ns  = [10, 20, 30, 40, 50]\nk_folds = 5\nitem_mae_values = item_item_recommendation(normalized_rating_matrix, k_folds ,top_ns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(user_mae_values, item_mae_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([10, 20, 30, 40, 50], user_mae_values, label = \"user-user\")\nplt.plot([10, 20, 30, 40, 50], item_mae_values, label = \"item-item\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_products = product_df.groupby('title')['overall'].sum().sort_values(ascending=False).head(10)\nprint(\"Top 10 products by user sum ratings:\")\nprint(top_products)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:13:19.847097Z","iopub.execute_input":"2024-04-07T13:13:19.847459Z","iopub.status.idle":"2024-04-07T13:13:19.927472Z","shell.execute_reply.started":"2024-04-07T13:13:19.847427Z","shell.execute_reply":"2024-04-07T13:13:19.926372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_products = product_df.groupby('asin')['overall'].sum().sort_values(ascending=False).head(10)\nprint(\"Top 10 products by user sum ratings:\")\nprint(top_products)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:29:50.356204Z","iopub.execute_input":"2024-04-07T14:29:50.357061Z","iopub.status.idle":"2024-04-07T14:29:50.401544Z","shell.execute_reply.started":"2024-04-07T14:29:50.35702Z","shell.execute_reply":"2024-04-07T14:29:50.400257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_products = df.groupby('asin')['overall'].sum().sort_values(ascending=False).head(10)\n\nprint(\"Top 10 products by user sum ratings:\")\nfor asin in top_products.index:\n    print(asin, meta_data_processed[meta_data_processed[\"asin\"] == asin][\"title\"].values[0],top_products[asin])","metadata":{},"execution_count":null,"outputs":[]}]}